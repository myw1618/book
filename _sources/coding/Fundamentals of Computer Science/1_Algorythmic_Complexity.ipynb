{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(algorythmic_complexity)=\n",
    "# Algorithmic Complexity\n",
    "``` {index} Algorithmic Complexity\n",
    "```\n",
    "\n",
    "In order to make our programs efficient (or, at least, not horribly inefficient), we can consider how the execution time varies depending on the input size \\\\(n\\\\). Let us define a measure of this efficiency as a function \\\\( T(n)\\\\). Of course, the time it takes to execute a code will vary largely depending on the processor, compiler or disk speed. \\\\( T(n)\\\\) goes around this variance by measuring *asymptotic* complexity. In this case, only (broadly defined) *steps* will determine the time it takes to execute an algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, say we want to add two \\\\(n\\\\)-bit binary digits. One way to do this is to go bit by bit and add the two. We can se that \\\\(n\\\\) operations are involved. \n",
    "\\\\[T(n) = c*n\\\\]\n",
    "where \\\\(c\\\\) is time it takes to add two bits on a machine. On different machines the value of \\\\(c\\\\) may vary, but the linearity of this function is the common factor. Our aim is to abstract away from the details of implementation and think about the fundamental usage of computing resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Oh Notation\n",
    "\n",
    "The mathematical definiton of this concept can be found [**here**](https://primer-computational-mathematics.github.io/book/mathematics/mathematical_notation/Big_O_notation.html). In simple terms, we say that:\n",
    "\n",
    "\\\\[f(n) = O(g(n))\\\\] \n",
    "if there exists \\\\(c>0\\\\) and \\\\(n_0>0\\\\) such that \n",
    "\n",
    "\\\\[f(n) <= c * g(n)\\\\]for all \\\\(n \\geq n_0\\\\).\n",
    "\n",
    "\\\\(g(n)\\\\) can be thought of as an *upper bound* of \\\\(f(n)\\\\) as \\\\(n\\\\) tends to infinity. Here are a couple of examples:\n",
    "\n",
    "\\\\[ 3n + 4 = O(n)\\\\]\n",
    "\\\\[ n^2 + 17n = O(n^2)\\\\]\n",
    "\\\\[2^n = O(2^n)\\\\]\n",
    "\\\\[42 = O(1)\\\\]\n",
    "but also:\n",
    "\n",
    "\\\\[log(n) = O(n)\\\\]\n",
    "\\\\[n = O(n^2)\\\\]\n",
    "\n",
    "We will now consider different time complexities of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Time \\\\(O(1)\\\\)\n",
    "\n",
    "And algorithm is said to run in *constant* time if its complexity is \\\\(O(1)\\\\). This is usually considered the *fastest* case (which is true, but only in the *asymptotic* case). No matter what the input size is, the program will take the same amount of time to terminate. Let us consider some examples:\n",
    "\n",
    "* ```Hello World!```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(n):\n",
    "    print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what ```n``` is, the function does the same thing, which takes the same time. Therefore its complexity is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
